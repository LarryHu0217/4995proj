{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d024930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88306787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee79ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be00c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/kaggle/input/waste-classification-data/DATASET/TRAIN/'\n",
    "data_path = r'data/DATASET/TRAIN'\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "#                     transforms.Resize([256, 256]),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c020418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(512, 1)    # replace the last full-connected layer of resnet\n",
    "        self.sigmoid = nn.Sigmoid()           # use a sigmoid layer to map the logit to [0, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.resnet(x)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7958c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541a8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the last two layers\n",
    "for param_name, param in model.named_parameters():\n",
    "    if 'fc' not in param_name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21af51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_func = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b134b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False  \n",
    "set_parameter_requires_grad(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(dataloaders):\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)            # forward propagation\n",
    "        loss = loss_func(outputs, labels)  # calculate loss\n",
    "        loss.backward()                    # back propagation\n",
    "        optimizer.step()                   # update parameters\n",
    "        optimizer.zero_grad()              # clear gradients\n",
    "        print('Epoch %s | %s / %s | loss: %s' % \n",
    "              (epoch, i+1, len(dataloaders), float(loss)), end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30454d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=25, is_train=True):\n",
    "    since = time.time()\n",
    "    \n",
    "    acc_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloaders:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "\n",
    "        acc_history.append(epoch_acc.item())\n",
    "        loss_history.append(epoch_loss)\n",
    "        \n",
    "#         torch.save(model.state_dict(), os.path.join('/kaggle/working', '{0:0=2d}.pth'.format(epoch)))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    return acc_history, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only want to update the gradient for the classifier layer that we initialized.\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "            \n",
    "optimizer = optim.Adam(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72563328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42744a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Setup the loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "train_acc_hist, train_loss_hist = train_model(model, train_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b8b7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
       "         [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
       "         [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
       "         [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
       "         [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
       "         [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
       "         [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]]),\n",
       " tensor([-0.0415,  0.0165,  0.0405, -0.0170,  0.0374,  0.0065,  0.0088,  0.0247,\n",
       "         -0.0099, -0.0220], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resnet.conv1.weight[0][0], model.resnet.fc.weight[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c6e03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_func = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e908b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | 706 / 706 | loss: 0.08934750407934189\n",
      "Epoch 1 | 706 / 706 | loss: 0.334738910198211675\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs) \n",
    "        outputs=outputs.to(torch.float32)\n",
    "        labels=labels.to(torch.float32)\n",
    "        loss = loss_func(outputs, labels)  # calculate loss\n",
    "        loss.backward()                    # back propagation\n",
    "        optimizer.step()                   # update parameters\n",
    "        optimizer.zero_grad()              # clear gradients\n",
    "        print('Epoch %s | %s / %s | loss: %s' % \n",
    "              (epoch, i+1, len(train_loader), float(loss)), end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bc8d11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f7c38faa970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73011d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accuracy score\n",
    "def accuracy_score(pred, y):\n",
    "    return (pred.astype(int) == y.astype(int)).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06cccdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 / 706\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9199645468646134"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "trues = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels = batch\n",
    "    outputs = model(inputs)\n",
    "    pred = (outputs > 0.5).detach().numpy()   # pred <= 0.5 -> cat;  pred > 0.5 -> dog \n",
    "    preds.append(pred)\n",
    "    trues.append(labels.numpy())\n",
    "    print('%s / %s' % (i+1, len(train_loader)), end='\\r')\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "accuracy_score(preds, trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "257b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        #keep-the-last-fully-connected-layer-of-resnetlf.resnet = resnet18(pretrained=True)\n",
    "        self.fc = nn.Linear(1000, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.resnet(x)\n",
    "        out = self.sigmoid(self.fc(out))\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfc09029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2756def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_func = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcc05b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | 706 / 706 | loss: 0.06546648591756825\n",
      "Epoch 1 | 706 / 706 | loss: 0.10812740027904512\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs) \n",
    "        outputs=outputs.to(torch.float32)\n",
    "        labels=labels.to(torch.float32)\n",
    "        loss = loss_func(outputs, labels)  # calculate loss\n",
    "        loss.backward()                    # back propagation\n",
    "        optimizer.step()                   # update parameters\n",
    "        optimizer.zero_grad()              # clear gradients\n",
    "        print('Epoch %s | %s / %s | loss: %s' % \n",
    "              (epoch, i+1, len(train_loader), float(loss)), end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff455d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 / 706\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8229115887436295"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "trues = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels = batch\n",
    "    outputs = model(inputs)\n",
    "    pred = (outputs > 0.5).detach().numpy()   # pred <= 0.5 -> cat;  pred > 0.5 -> dog \n",
    "    preds.append(pred)\n",
    "    trues.append(labels.numpy())\n",
    "    print('%s / %s' % (i+1, len(train_loader)), end='\\r')\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "accuracy_score(preds, trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4ec2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 706 / 706\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "trues = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels = batch\n",
    "    outputs = model(inputs)\n",
    "    pred = (outputs > 0.5).detach().numpy()   # pred <= 0.5 -> cat;  pred > 0.5 -> dog \n",
    "    preds.append(pred)\n",
    "    trues.append(labels.numpy())\n",
    "    print('%s / %s' % (i+1, len(train_loader)), end='\\r')\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "accuracy_score(preds, trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ec07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
